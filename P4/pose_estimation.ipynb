{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "DDJwQPZcupab",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# ROB 498-002/599-009 Project 4: Pose Estimation - PoseCNN\n",
    "\n",
    "Before we start, please put your name and UMID in following format\n",
    "\n",
    ": Firstname LASTNAME, #00000000   //   e.g.) Anthony OPIPARI, #12345678"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "JSXasOiouZdl",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**Your Answer:**   \n",
    "Firstname Lastname, #UMID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "PoRTyUc94S1a",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# PoseCNN: A Classic end-to-end pose estimation network\n",
    "\n",
    "In this exercise you will implement an **end-to-end** object pose estimator, based on [PoseCNN](https://arxiv.org/abs/1711.00199), which consists of two stages - feature extraction with a backbone network and pose estimation represented by instance segmentation, 3D translation estimation, and 3D rotation estimation.\n",
    "We will train it to estimate the pose of a set of object classes and evaluate the estimation accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "LfBk3NtRgqaV",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "ubB_0e-UAOVK",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Setup Code\n",
    "Before getting started, we need to run some boilerplate code to set up our environment, same as previous assignments. You'll need to rerun this setup code each time you start the notebook.\n",
    "\n",
    "First, run this cell load the autoreload extension. This allows us to edit .py source files, and re-import them into the notebook for a seamless editing and debugging experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1678887695327,
     "user": {
      "displayName": "Anthony Opipari",
      "userId": "13512160930954150621"
     },
     "user_tz": 240
    },
    "id": "ASkY27ZtA7Is",
    "jupyter": {
     "outputs_hidden": true
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "MzqbYcKdz6ew",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Google Colab Setup\n",
    "Next we need to run a few commands to set up our environment on Google Colab. If you are running this notebook on a local machine you can skip this section.\n",
    "\n",
    "Run the following cell to mount your Google Drive. Follow the link, sign in to your Google account (the same account you used to store this notebook!) and copy the authorization code into the text box that appears below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34462,
     "status": "ok",
     "timestamp": 1678887729782,
     "user": {
      "displayName": "Anthony Opipari",
      "userId": "13512160930954150621"
     },
     "user_tz": 240
    },
    "id": "HzRdJ3uhe1CR",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "outputId": "bb87f851-4efa-47bc-8d4f-9ce181ad9d09",
    "run_control": {
     "read_only": false
    },
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "OvUDZWGU3VLV",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now recall the path in your Google Drive where you uploaded this notebook, fill it in below. If everything is working correctly then running the following cell should print the filenames from the assignment:\n",
    "\n",
    "```\n",
    "[\"p4_helper.py\", \"rob599\", \"pose_cnn.py\", \"pose_estimation.ipynb\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1575,
     "status": "ok",
     "timestamp": 1678887779237,
     "user": {
      "displayName": "Anthony Opipari",
      "userId": "13512160930954150621"
     },
     "user_tz": 240
    },
    "id": "RrAX9FOLpr9k",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "outputId": "3cbc79db-8b98-4d2b-d014-b7a292c60ac5",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# TODO: Fill in the Google Drive path where you uploaded the assignment\n",
    "# Example: If you create a 2023WN folder and put all the files under P4 folder, then \"2023WN/P4\"\n",
    "# GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = '2023WN/P4'\n",
    "GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = None\n",
    "GOOGLE_DRIVE_PATH = os.path.join(\"drive\", \"My Drive\", GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
    "print(os.listdir(GOOGLE_DRIVE_PATH))\n",
    "\n",
    "\n",
    "# Add to sys so we can import .py files.\n",
    "sys.path.append(GOOGLE_DRIVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XkhvDeZFGyNB"
   },
   "source": [
    "Next, we install a couple packages to help with processing and visualizing object models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25203,
     "status": "ok",
     "timestamp": 1678887808398,
     "user": {
      "displayName": "Anthony Opipari",
      "userId": "13512160930954150621"
     },
     "user_tz": 240
    },
    "id": "76xg6TA-GSAZ",
    "outputId": "58455679-cd05-44e1-ae2c-272534856e8e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYOPENGL_PLATFORM'] = 'egl'\n",
    "\n",
    "%pip install trimesh\n",
    "%pip install pyrender\n",
    "%pip install pyquaternion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "RldDumJE48pv",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Once you have successfully mounted your Google Drive and located the path to this assignment, run the following cell to allow us to import from the `.py` files of this assignment. If it works correctly, it should print the message:\n",
    "\n",
    "```\n",
    "Hello from pose_cnn.py!\n",
    "Hello from p4_helper.py!\n",
    "```\n",
    "\n",
    "as well as the last edit time for the file `pose_cnn.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14038,
     "status": "ok",
     "timestamp": 1678887838113,
     "user": {
      "displayName": "Anthony Opipari",
      "userId": "13512160930954150621"
     },
     "user_tz": 240
    },
    "id": "pTIwSpkS495_",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "outputId": "e29b1ac6-dd9b-4696-d964-302dd5b5f42c",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "os.environ[\"TZ\"] = \"US/Eastern\"\n",
    "time.tzset()\n",
    "\n",
    "from pose_cnn import hello_pose_cnn\n",
    "from p4_helper import hello_helper\n",
    "\n",
    "\n",
    "hello_pose_cnn()\n",
    "hello_helper()\n",
    "\n",
    "pose_cnn_path = os.path.join(GOOGLE_DRIVE_PATH, \"pose_cnn.py\")\n",
    "pose_cnn_edit_time = time.ctime(\n",
    "    os.path.getmtime(pose_cnn_path)\n",
    ")\n",
    "print(\"pose_cnn.py last edited on %s\" % pose_cnn_edit_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "GWP1vCGL5Eca",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Load several useful packages that are used in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "executionInfo": {
     "elapsed": 2366,
     "status": "ok",
     "timestamp": 1678887842345,
     "user": {
      "displayName": "Anthony Opipari",
      "userId": "13512160930954150621"
     },
     "user_tz": 240
    },
    "id": "CwVZ26yM5G8U",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from p4_helper import *\n",
    "from rob599 import reset_seed\n",
    "from rob599.grad import rel_error\n",
    "\n",
    "# for plotting\n",
    "plt.rcParams[\"figure.figsize\"] = (10.0, 8.0)  # set default size of plots\n",
    "plt.rcParams[\"font.size\"] = 16\n",
    "plt.rcParams[\"image.interpolation\"] = \"nearest\"\n",
    "plt.rcParams[\"image.cmap\"] = \"gray\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "x7poKGI35JZY",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We will use GPUs to accelerate our computation in this notebook. Run the following to make sure GPUs are enabled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 984,
     "status": "ok",
     "timestamp": 1678887844659,
     "user": {
      "displayName": "Anthony Opipari",
      "userId": "13512160930954150621"
     },
     "user_tz": 240
    },
    "id": "Vw3wIuCu5LnU",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "outputId": "21d0fdde-a89b-4308-b126-458abb102a36",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Good to go!\")\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"Please set GPU via Edit -> Notebook Settings.\")\n",
    "    DEVICE = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "MjJ3uyYBg3Lw",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Load PROPS Pose Dataset\n",
    "During the majority of our homework assignments so far, we have used the PROPS Classification or Detection datasets for image processing tasks.\n",
    "\n",
    "In order to train and evaluate object pose estimation models, we need a dataset where each image is annotated with a *set* of *pose labels*, where each pose label gives the 3DoF position and 3DoF orientation of some object in the image.\n",
    "\n",
    "We will use the [PROPS Pose](https://deeprob.org/datasets/props-pose/) dataset, which provides annotations of this form. \n",
    "Our PROPS Detection dataset is much smaller than typical benchmarking pose estimation datasets, and thus easier to manage in an homework assignment.\n",
    "PROPS comprises annotated bounding boxes for 10 object classes:\n",
    "`[\"master_chef_can\", \"cracker_box\", \"sugar_box\", \"tomato_soup_can\", \"mustard_bottle\", \"tuna_fish_can\", \"gelatin_box\", \"potted_meat_can\", \"mug\", \"large_marker\"]`.\n",
    "The choice of these objects is inspired by the [YCB object and Model set](https://ieeexplore.ieee.org/document/7251504) commonly used in robotic perception models.\n",
    "\n",
    "We create a [`PyTorch Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) class\n",
    "named `PROPSPoseDataset` in `rob599/PROPSPoseDataset.py` that will download the PROPS Pose dataset.\n",
    "\n",
    "Run the following two cells to set a few config parameters and then download the train/val sets for the PROPS Pose dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 962,
     "status": "ok",
     "timestamp": 1678887859541,
     "user": {
      "displayName": "Anthony Opipari",
      "userId": "13512160930954150621"
     },
     "user_tz": 240
    },
    "id": "AetPkU4wEmlm"
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "# Set a few constants related to data loading.\n",
    "NUM_CLASSES = 10\n",
    "BATCH_SIZE = 4\n",
    "NUM_WORKERS = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86,
     "referenced_widgets": [
      "a7314e8a8bf04ca0b075190585f02587",
      "68e331529bc047a980305ee37d367b63",
      "13db67adde444f5082c74fea15428661",
      "c5c087128f7c4bd3a5e5b0377b06ce21",
      "b6c0da1c5e69422b819ba72f940c40bb",
      "5cbc97e6d1074e9f94d44f7730469d39",
      "7b8f279be8e24b64b4abbb74df67be47",
      "21dc60d7d0104670b2c01a1e8730761f",
      "ac85c80f892048649365ba2767bfe19a",
      "6a458396ab194fc59d149d5b9ebffa2a",
      "ce568e54621a43b383fb0d89a0c6bddd"
     ]
    },
    "id": "BJe14F-D5Cgk",
    "outputId": "ca34d656-d287-4288-f536-5daaffe15b50"
   },
   "outputs": [],
   "source": [
    "from rob599 import PROPSPoseDataset\n",
    " \n",
    "# NOTE: Set `download=True` for the first time when you set up Google Drive folder.\n",
    "# Turn it back to `False` later for faster execution in the future.\n",
    "# If this hangs, download and place data in your drive manually.\n",
    "train_dataset = PROPSPoseDataset(\n",
    "    GOOGLE_DRIVE_PATH, \"train\",\n",
    "    download=False  # True (for the first time)\n",
    ") \n",
    "val_dataset = PROPSPoseDataset(GOOGLE_DRIVE_PATH, \"val\")\n",
    "\n",
    "print(f\"Dataset sizes: train ({len(train_dataset)}), val ({len(val_dataset)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset will format each sample from the dataset as a dictionary containing the following keys:\n",
    "\n",
    " - 'rgb': a numpy float32 array of shape (3, 480, 640) scaled to range [0,1]\n",
    " - 'depth': a numpy int32 array of shape (1, 480, 640) in (mm)\n",
    " - 'objs_id': a numpy uint8 array of shape (10,) containing integer ids for visible objects (1-10) and invisible objects (0)\n",
    " - 'label': a numpy bool array of shape (11, 480, 640) containing instance segmentation for objects in the scene\n",
    " - 'bbx': a numpy float64 array of shape (10, 4) containing (x, y, w, h) coordinates of object bounding boxes\n",
    " - 'RTs': a numpy float64 array of shape (10, 3, 4) containing homogeneous transformation matrices per object into camera coordinate frame\n",
    " - 'centermaps': a numpy float64 array of shape (30, 480, 640) containing (dx, dy, z) coordinates to each object's centroid \n",
    " - 'centers': a numpy float64 array of shape (10, 2) containing (x, y) coordinates of object centroids projected to image plane \n",
    " \n",
    "This dataset assumes that the upper left of the image is the origin point (0, 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Dataset\n",
    "\n",
    "Now let's visualize a few samples from our validation set to make sure the images and labels are loaded correctly. In this next cell, we'll use the `visualize_dataset` function from `rob599/utils.py` to view the RGB observation and labeled pose labels for three random samples. \n",
    "\n",
    "In the below figure, the final column plots the centermaps for class 0, which corresponds to the master chef coffee can. This plot is included to give a sense of how the centermaps represent gradients towards the object's centroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from rob599 import reset_seed, visualize_dataset\n",
    "\n",
    "reset_seed(0)\n",
    "\n",
    "grid_vis = visualize_dataset(val_dataset,alpha = 0.25)\n",
    "plt.axis('off')\n",
    "plt.imshow(grid_vis)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing PoseCNN\n",
    "\n",
    "Now that we have our dataset loaded and ready to use, we'll begin implementing a variant of the [PoseCNN](https://arxiv.org/abs/1711.00199) network. This architecture is designed to take an RGB color image as input and produce a [6 degrees-of-freedom pose](https://en.wikipedia.org/wiki/Six_degrees_of_freedom) estimate for each instance of an object within the scene from which the image was taken. To do this, PoseCNN uses 5 operations within the architecture. First, a backbone convolutional feature extraction network is used to produce a tensor representing learned features from the input image. Second, the extracted features are processed by an embedding branch to reduce the spatial resolution and memory overhead for downstream layers. Third, an instance segmentation branch uses the embedded features to identify regions in the image corresponding to each object instance (regions of interest). Fourth, the translations for each object instance are estimated using a translation branch along with the embedded features. Finally, a rotation branch uses the embedded features to estimate a rotation, in the form of a [quaternion](https://en.wikipedia.org/wiki/Quaternions_and_spatial_rotation), for each region of interest.\n",
    "\n",
    "Thr architecture is shown in more detail from Figure 2 of the [PoseCNN paper](https://arxiv.org/abs/1711.00199):\n",
    "\n",
    "![architecture](https://deeprob.org/assets/images/posecnn_arch.png)\n",
    "\n",
    "Now, we will implement a variant of this architecture that performs each of the 5 operations using PyTorch and data from our `PROPSPoseDataset`. The remainder of the features for this project will be implemented in the `pose_cnn.py` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the Backbone and Feature Extraction Branch\n",
    "\n",
    "In the past project, you implemented a deep convolutional network from scratch. In this project, we'll use [torchvision's](https://pytorch.org/vision/stable/index.html) pretrained convolutional networks for our backbone convolutional feature extractor. Specifically, we'll use the [VGG16 model](https://arxiv.org/abs/1409.1556) as our feature extractor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "vgg16 = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the definition of this model by referring to the [model's source code within torchvision](https://pytorch.org/vision/main/_modules/torchvision/models/vgg.html). From this definition, we can see that the `vgg16.features` variable stores the feature extraction portion of VGG16 before average pooling. We'll use this pretrained VGG16 model for our PoseCNN model's pretrained extraction backbone. Print out the layers in `vgg16.features` by running the code cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this `vgg16` model as the `pretrained_network`, we implement PoseCNN's feature extraction branch for you in the `FeatureExtraction` class of `pose_cnn.py`. You should inspect this function to understand how the output `feature1` and `feature2` tensors are created. In addition, observe that we freeze the early layers of our pretrained network by setting the corresponding convolutional weights and bias to have `requires_grad=False`. Freezing these layers will speed up training and help reduce overfitting.\n",
    "\n",
    "We can inspect the expected sizes of `feature1` and `feature2` by passing the `FeatureExtraction` dummy input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from pose_cnn import FeatureExtraction\n",
    "\n",
    "# Based on PoseCNN section III.B, the output features should \n",
    "# be 1/8 and 1/16 the input's spatial resolution with 512 channels\n",
    "print('feature1 expected shape: (N, {}, {}, {})'.format(512, 480//8, 640//8))\n",
    "print('feature2 expected shape: (N, {}, {}, {})'.format(512, 480//16, 640//16))\n",
    "print()\n",
    "\n",
    "vgg16 = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
    "feature_extractor = FeatureExtraction(pretrained_model=vgg16)\n",
    "\n",
    "dummy_input = {'rgb': torch.zeros((2,3,480,640))}\n",
    "feature1, feature2 = feature_extractor(dummy_input)\n",
    "\n",
    "print('feature1 shape:', feature1.shape)\n",
    "print('feature2 shape:', feature2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on to the embedding and segmentation branch, take a moment to connect our `feature1` and `feature2` variables with the PoseCNN figure 2 shown earlier in the notebook. Understanding which arrows corresponds to `feature1` and `feature2` will make the remaining networks more comprehensible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Segmentation Branch\n",
    "\n",
    "Now that we have our feature extractor setup, we'll implement the instnace segmentation branch. This branch should fuse information from the feature extractor (`feature1` and `feature2`) according to the architecture diagram of PoseCNN. Specifically, the network will pass both outputs from the feature extractor through a 1x1 convolution+ReLU layer followed by interpolation and an element wise addition. Next these intermediate features are interpolated back to the input image size followed by a final 1x1 convolution+ReLU layer to predict a probability for each class or background at each pixel. Follow the instructions in the `SegmentationBranch` class of `pose_cnn.py` to implement this branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from rob599 import reset_seed\n",
    "from pose_cnn import FeatureExtraction, SegmentationBranch\n",
    "\n",
    "reset_seed(0)\n",
    "\n",
    "vgg16 = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
    "feature_extractor = FeatureExtraction(pretrained_model=vgg16)\n",
    "segmentation_branch = SegmentationBranch()\n",
    "\n",
    "dummy_input = {'rgb': torch.zeros((2,3,480,640))}\n",
    "feature1, feature2 = feature_extractor(dummy_input)\n",
    "probability, segmentation, bbx = segmentation_branch(feature1, feature2)\n",
    "\n",
    "print('probability expected shape: (B, 11, 480, 640)')\n",
    "print('segmentation expected shape: (B, 480, 640)')\n",
    "print('bbx expected shape: (N, 6) (where N is the number of rois extracted from the predicted segmentation)')\n",
    "print()\n",
    "print('probability shape:', probability.shape)\n",
    "print('segmentation shape:', segmentation.shape)\n",
    "print('bbx shape:', bbx.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Instance Segmentation Branch to PoseCNN\n",
    "\n",
    "Now that we have our instance segmentation branch implemented, we can begin building up our `PoseCNN` class. In `pose_cnn.py` use the given `FeatureExtraction` class and your implemented `SegmentationBranch` to initialize the extraction and segmentation portion of PoseCNN. Next, implement the training and testing time forward pass of `PoseCNN` to perform only instance segmentation. We will come back and add rotation+translation branches once we are confident in our instance segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training PoseCNN to Perform Instance Segmentation\n",
    "\n",
    "Once you've added code to initialize and perform the forward pass of PoseCNN for feature extraction and instance segmentation, we can attempt to train this part of PoseCNN by itself. The code in the following cell will initialize a PoseCNN model and begin training it on instance segmentation only. You should expect to see your training loss decrease to ~0.1 after training for 2 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "\n",
    "from rob599 import reset_seed\n",
    "from pose_cnn import PoseCNN\n",
    "from tqdm import tqdm\n",
    "\n",
    "reset_seed(0)\n",
    "\n",
    "vgg16 = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1).to(DEVICE)\n",
    "posecnn_model = PoseCNN(pretrained_backbone = vgg16,\n",
    "                       models_pcd = torch.tensor(train_dataset.models_pcd).to(DEVICE, dtype=torch.float32),\n",
    "                       cam_intrinsic = train_dataset.cam_intrinsic).to(DEVICE)\n",
    "\n",
    "dataloader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE)\n",
    "optimizer = torch.optim.Adam(posecnn_model.parameters(), lr=0.001,\n",
    "                                 betas=(0.9, 0.999))\n",
    "\n",
    "posecnn_model.train()\n",
    "\n",
    "loss_history = []\n",
    "log_period = 5\n",
    "_iter = 0\n",
    "\n",
    "st_time = time.time()\n",
    "for epoch in range(3):\n",
    "    train_loss = []\n",
    "    for batch in tqdm(dataloader):\n",
    "        for item in batch:\n",
    "            batch[item] = batch[item].to(DEVICE)\n",
    "        loss_dict = posecnn_model(batch)\n",
    "        optimizer.zero_grad()\n",
    "        total_loss = loss_dict[\"loss_segmentation\"]\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(total_loss.item())\n",
    "    \n",
    "        if _iter % log_period == 0:\n",
    "            loss_history.append(total_loss.item())\n",
    "        _iter += 1\n",
    "    \n",
    "    print('Time {0}'.format(time.strftime(\"%Hh %Mm %Ss\", time.gmtime(time.time() - st_time)) + \\\n",
    "                                  ', ' + 'Epoch %02d' % epoch + ', ' + 'Training finished' + f' , with mean training loss {np.array(train_loss).mean()}'))\n",
    "    \n",
    "plt.title(\"Training loss history\")\n",
    "plt.xlabel(f\"Iteration (x {log_period})\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(loss_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference for Instance Segmentation\n",
    "\n",
    "Now that we have our segmentation network trained, we can qualitatively evaluate the segmentation results. The following notebook cell will visualize output segmentations on a sample from the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "\n",
    "reset_seed(0)\n",
    "\n",
    "num_samples = 3\n",
    "posecnn_model.eval()\n",
    "\n",
    "plt.text(300, -40, 'RGB', ha=\"center\")\n",
    "plt.text(950, -40, 'True\\nSegmentation', ha=\"center\")\n",
    "plt.text(1600, -40, 'Predicted\\nSegmentation', ha=\"center\")\n",
    "\n",
    "samples = []\n",
    "for sample_i in range(num_samples):\n",
    "    sample_idx = random.randint(0,len(val_dataset)-1)\n",
    "    sample = val_dataset[sample_idx]\n",
    "    \n",
    "    rgb = torch.tensor(sample['rgb'][None, :]).to(DEVICE)\n",
    "    _, prediction = posecnn_model({'rgb': rgb})\n",
    "    prediction = prediction.cpu().numpy().astype(np.float64)\n",
    "    prediction /= prediction.max()\n",
    "    prediction = (np.tile(prediction, (3, 1, 1)) * 255).astype(np.uint8)\n",
    "    rgb = (sample['rgb'].transpose(1, 2, 0) * 255).astype(np.uint8)\n",
    "    depth = ((np.tile(sample['depth'], (3, 1, 1)) / sample['depth'].max()) * 255).astype(np.uint8)\n",
    "    segmentation = (sample['label']*np.arange(11).reshape((11,1,1))).sum(0,keepdims=True).astype(np.float64)\n",
    "    segmentation /= segmentation.max()\n",
    "    segmentation = (np.tile(segmentation, (3, 1, 1)) * 255).astype(np.uint8)\n",
    "    \n",
    "    samples.append(torch.tensor(rgb.transpose(2, 0, 1)))\n",
    "    samples.append(torch.tensor(segmentation))\n",
    "    samples.append(torch.tensor(prediction))\n",
    "\n",
    "img = make_grid(samples, nrow=3).permute(1, 2, 0)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, visually inspect the segmentation results above to ensure your forward functions and loss calculations are setup correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the Translation Branch\n",
    "\n",
    "Now that we have our feature extractor and instance segmentation working, we'll implement the rotation and translation branches of PoseCNN. We suggest starting with the translation branch, which follows a similar embedding structure as was used for instance segmentation. Based on the architecture described in the paper, implement your translation network in the `TranslationBranch` class of `pose_cnn.py`.\n",
    "\n",
    "Once you have implemented `TranslationBranch`, the following notebook cell can be used to verify the shape of your output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from rob599 import reset_seed\n",
    "from pose_cnn import FeatureExtraction, TranslationBranch\n",
    "\n",
    "reset_seed(0)\n",
    "\n",
    "vgg16 = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
    "feature_extractor = FeatureExtraction(pretrained_model=vgg16)\n",
    "translation_branch = TranslationBranch()\n",
    "\n",
    "dummy_input = {'rgb': torch.zeros((2,3,480,640))}\n",
    "feature1, feature2 = feature_extractor(dummy_input)\n",
    "translation = translation_branch(feature1, feature2)\n",
    "\n",
    "print('translation expected shape: (2, 30, 480, 640)')\n",
    "print()\n",
    "print('translation shape:', translation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the Rotation Branch\n",
    "\n",
    "Now you can implement the final module of PoseCNN: the rotation branch. This portion of PoseCNN will be reminiscient of fasterRCNN in that we will predict a quaternion for each possible class at each region of interest detected by our preceeding segmentation branch. To do this, we'll need to use ROIPooling for feature extraction. Since you implemented ROIPooling in the last project, feel free to use the [ROIPool](https://pytorch.org/vision/main/generated/torchvision.ops.RoIPool.html) implemented by `torchvision.ops` for this project.\n",
    "\n",
    "Once you have implemented `RotationBranch`, the following notebook cell can be used to verify the shape of your output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from rob599 import reset_seed\n",
    "from pose_cnn import FeatureExtraction, RotationBranch\n",
    "\n",
    "reset_seed(0)\n",
    "\n",
    "vgg16 = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
    "feature_extractor = FeatureExtraction(pretrained_model=vgg16)\n",
    "rotation_branch = RotationBranch()\n",
    "\n",
    "dummy_input = {'rgb': torch.zeros((2,3,480,640))}\n",
    "dummy_bbx = torch.zeros((10,5)) # N = 10\n",
    "feature1, feature2 = feature_extractor(dummy_input)\n",
    "quaternion = rotation_branch(feature1, feature2, dummy_bbx)\n",
    "\n",
    "print('quaternion expected shape: (N, 40)')\n",
    "print()\n",
    "print('quaternion shape:', quaternion.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hough Voting Layer\n",
    "\n",
    "One important piece of the PoseCNN architecture for inference time that we haven't implemented yet is a Hough voting layer. As described in the text, and illustrated below, a Hough voting layer is used during inference time to extract a single centroid prediction from the translation maps produced by `TranslationBranch` and the segments produced by `SegmentationBranch`. Due to the timing constraints of our semester, we have implemented this Hough voting layer for you in `P4_helper.py` as the `HoughVoting` function. We recommend you use our provided `HoughVoting` function after reading through its implementation to understand how it works\n",
    "\n",
    "![hough](https://deeprob.org/assets/images/posecnn_hough.png)\n",
    "\n",
    "We also recommend reading through other given helper functions in `p4_helper.py` and the class definition of `PoseCNN` in `pose_cnn.py`. **Some of these utilities will be helpful for completing your PoseCNN implementation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together: PoseCNN\n",
    "\n",
    "We now have all the modules needed to make up our PoseCNN architecture. In the `PoseCNN` class of `pose_cnn.py`, add the translation and rotation branches to the initialization and forward functions. During training, your PoseCNN model should output a `loss_dict` variable with loss values for segmentation, translation and rotation branches stored respectively with keys of `\"loss_segmentation\"`, `\"loss_centermap\"`, and `\"loss_R\"`. The segmentation loss should be calculated using `p4_helper.loss_cross_entropy`, the centroid loss should be calculated using l1Loss, and the rotation loss should be calculated using the provided helper in `p4_helper.loss_Rotation`. During inference, your model should output a dictionary of predicted poses (i.e. see `PoseCNN.generate_pose` for a formatting utility) in `output_dict` and the predicted segmentation map (post processed probabilities) in `segmentation`.\n",
    "\n",
    "After this, you will be ready to train your PoseCNN model with all three losses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "\n",
    "import rob599\n",
    "from pose_cnn import PoseCNN\n",
    "\n",
    "rob599.reset_seed(0)\n",
    "\n",
    "dataloader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "vgg16 = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
    "posecnn_model = PoseCNN(pretrained_backbone = vgg16, \n",
    "                models_pcd = torch.tensor(train_dataset.models_pcd).to(DEVICE, dtype=torch.float32),\n",
    "                cam_intrinsic = train_dataset.cam_intrinsic).to(DEVICE)\n",
    "posecnn_model.train()\n",
    "    \n",
    "optimizer = torch.optim.Adam(posecnn_model.parameters(), lr=0.001,\n",
    "                            betas=(0.9, 0.999))\n",
    "\n",
    "\n",
    "loss_history = []\n",
    "log_period = 5\n",
    "_iter = 0\n",
    "\n",
    "\n",
    "st_time = time.time()\n",
    "for epoch in range(10):\n",
    "    train_loss = []\n",
    "    dataloader.dataset.dataset_type = 'train'\n",
    "    for batch in dataloader:\n",
    "        for item in batch:\n",
    "            batch[item] = batch[item].to(DEVICE)\n",
    "        loss_dict = posecnn_model(batch)\n",
    "        optimizer.zero_grad()\n",
    "        total_loss = 0\n",
    "        for loss in loss_dict:\n",
    "            total_loss += loss_dict[loss]\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(total_loss.item())\n",
    "        \n",
    "        if _iter % log_period == 0:\n",
    "            loss_str = f\"[Iter {_iter}][loss: {total_loss:.3f}]\"\n",
    "            for key, value in loss_dict.items():\n",
    "                loss_str += f\"[{key}: {value:.3f}]\"\n",
    "\n",
    "            print(loss_str)\n",
    "            loss_history.append(total_loss.item())\n",
    "        _iter += 1\n",
    "        \n",
    "    print('Time {0}'.format(time.strftime(\"%Hh %Mm %Ss\", time.gmtime(time.time() - st_time)) + \\\n",
    "                                  ', ' + 'Epoch %02d' % epoch + ', ' + 'Training finished' + f' , with mean training loss {np.array(train_loss).mean()}'))    \n",
    "\n",
    "torch.save(posecnn_model.state_dict(), os.path.join(GOOGLE_DRIVE_PATH, \"posecnn_model.pth\"))\n",
    "    \n",
    "plt.title(\"Training loss history\")\n",
    "plt.xlabel(f\"Iteration (x {log_period})\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(loss_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "id": "KhWZT-ztEaqm",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Inference\n",
    "\n",
    "Visualize a few outputs from the full trained model. These could be improved if we used a larger model, trained for greater duration, and if we used ICP with depth data to refine the final estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "id": "J7ArGiLTnHta",
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "outputId": "c4f829e4-50dd-47c2-eec6-90727593aae9",
    "run_control": {
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "\n",
    "import rob599\n",
    "from pose_cnn import PoseCNN, eval\n",
    "\n",
    "\n",
    "rob599.reset_seed(0)\n",
    "\n",
    "dataloader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "vgg16 = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
    "posecnn_model = PoseCNN(pretrained_backbone = vgg16, \n",
    "                models_pcd = torch.tensor(val_dataset.models_pcd).to(DEVICE, dtype=torch.float32),\n",
    "                cam_intrinsic = val_dataset.cam_intrinsic).to(DEVICE)\n",
    "posecnn_model.load_state_dict(torch.load(os.path.join(GOOGLE_DRIVE_PATH, \"posecnn_model.pth\")))\n",
    "\n",
    "num_samples = 5\n",
    "for i in range(num_samples):\n",
    "    out = eval(posecnn_model, dataloader, DEVICE)\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.imshow(out)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's measure the quantitative accuracy of our trained model using the 5°5cm metric. That is, we'll count how many visible objects our model was able to predict correctly, where a correct prediction is defined as one with a rotation error of less than 5° and a translation error of less than 5cm.\n",
    "\n",
    "The instructor's model, trained with the hyperparameters above achieves 29.3%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "\n",
    "import pyquaternion\n",
    "from tqdm import tqdm\n",
    "\n",
    "import rob599\n",
    "from pose_cnn import PoseCNN\n",
    "\n",
    "rob599.reset_seed(0)\n",
    "\n",
    "dataloader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "posecnn_model.load_state_dict(torch.load(os.path.join(GOOGLE_DRIVE_PATH, \"posecnn_model.pth\")))\n",
    "posecnn_model.eval()\n",
    "\n",
    "\n",
    "T_thresh = 5 # cm\n",
    "R_thresh = 5 # deg\n",
    "\n",
    "total =0\n",
    "correct = 0\n",
    "for batch in tqdm(dataloader):\n",
    "    for item in batch:\n",
    "        batch[item] = batch[item].to(DEVICE)\n",
    "    pose_dict, segmentation = posecnn_model(batch)\n",
    "    for bidx in range(BATCH_SIZE):\n",
    "        objs_visib = batch['objs_id'][bidx].cpu().tolist()\n",
    "        objs_preds = sorted(list(pose_dict[bidx].keys()))\n",
    "        for objidx, objs_id in enumerate(objs_visib):\n",
    "            if objs_id==0:\n",
    "                continue\n",
    "\n",
    "            total += 1\n",
    "            if objs_id not in objs_preds:\n",
    "                continue\n",
    "            RT_pred = pose_dict[bidx][objs_id]\n",
    "            RT_true = batch['RTs'][bidx][objidx].cpu().numpy()\n",
    "\n",
    "            # Translation error\n",
    "            T_pred = RT_pred[:3,3]\n",
    "            T_true = RT_true[:3,3]\n",
    "            T_err = 100*np.linalg.norm(T_pred-T_true) # error in cm\n",
    "\n",
    "            # Rotation error\n",
    "            R_true = pyquaternion.Quaternion(matrix=RT_true[:3,:3],atol=1e-6)\n",
    "            R_pred = pyquaternion.Quaternion(matrix=RT_pred[:3,:3],atol=1e-6)\n",
    "\n",
    "            R_rel = R_pred * R_true.conjugate\n",
    "            R_err = math.degrees(R_rel.angle)\n",
    "\n",
    "            if T_err<T_thresh and R_err<R_thresh:\n",
    "                correct+=1\n",
    "\n",
    "print(\"Accuracy at 5°5cm:\",correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit Your Work\n",
    "After completing this notebook, run the following cell to create a `.zip` file for you to download and turn in. \n",
    "\n",
    "**Please MANUALLY SAVE every `*.ipynb` and `*.py` files before executing the following cell:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rob599.submit import make_p4_submission\n",
    "\n",
    "# TODO: Replace these with your actual uniquename and umid\n",
    "uniquename = None\n",
    "umid = None\n",
    "\n",
    "make_p4_submission(GOOGLE_DRIVE_PATH, uniquename, umid)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "interpreter": {
   "hash": "4ef42baa288ce895b984811292da1481faa2138d6a325169bc8d9d38d49f8a2b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "13db67adde444f5082c74fea15428661": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_21dc60d7d0104670b2c01a1e8730761f",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ac85c80f892048649365ba2767bfe19a",
      "value": 1
     }
    },
    "21dc60d7d0104670b2c01a1e8730761f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "5cbc97e6d1074e9f94d44f7730469d39": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "68e331529bc047a980305ee37d367b63": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5cbc97e6d1074e9f94d44f7730469d39",
      "placeholder": "​",
      "style": "IPY_MODEL_7b8f279be8e24b64b4abbb74df67be47",
      "value": ""
     }
    },
    "6a458396ab194fc59d149d5b9ebffa2a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7b8f279be8e24b64b4abbb74df67be47": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a7314e8a8bf04ca0b075190585f02587": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_68e331529bc047a980305ee37d367b63",
       "IPY_MODEL_13db67adde444f5082c74fea15428661",
       "IPY_MODEL_c5c087128f7c4bd3a5e5b0377b06ce21"
      ],
      "layout": "IPY_MODEL_b6c0da1c5e69422b819ba72f940c40bb"
     }
    },
    "ac85c80f892048649365ba2767bfe19a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b6c0da1c5e69422b819ba72f940c40bb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c5c087128f7c4bd3a5e5b0377b06ce21": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6a458396ab194fc59d149d5b9ebffa2a",
      "placeholder": "​",
      "style": "IPY_MODEL_ce568e54621a43b383fb0d89a0c6bddd",
      "value": " 607676303/? [00:16&lt;00:00, 51655429.70it/s]"
     }
    },
    "ce568e54621a43b383fb0d89a0c6bddd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
